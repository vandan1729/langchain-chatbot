from datetime import datetime

import openai
import streamlit as st
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI
from streamlit.logger import get_logger

logger = get_logger("Langchain-Chatbot")


# decorator
def enable_chat_history(func):
    # to clear chat history after switching chatbot
    current_page = func.__qualname__
    if "current_page" not in st.session_state:
        st.session_state["current_page"] = current_page
    if st.session_state["current_page"] != current_page:
        try:
            st.cache_resource.clear()
            del st.session_state["current_page"]
            del st.session_state["messages"]
        except Exception as e:
            logger.error(f"Error clearing session state: {e}")

    # to show chat history on ui
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]

    for msg in st.session_state["messages"]:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])
            if msg["role"] == "assistant" and "llm_model" in msg:
                st.caption(f"*Generated by: {msg['llm_model']}*")

    def execute(*args, **kwargs):
        func(*args, **kwargs)

    return execute


def display_msg(msg, author, llm_model=None):
    """Method to display message on the UI

    Args:
        msg (str): message to display
        author (str): author of the message -user/assistant
        llm_model (str): the LLM model used for assistant messages
    """
    message_data = {"role": author, "content": msg}
    if author == "assistant" and llm_model:
        message_data["llm_model"] = llm_model

    st.session_state.messages.append(message_data)

    # Display message on UI
    with st.chat_message(author):
        st.write(msg)
        if author == "assistant" and llm_model:
            st.caption(f"*Generated by: {llm_model}*")


def choose_custom_openai_key():
    openai_api_key = st.sidebar.text_input(
        label="OpenAI API Key", type="password", placeholder="sk-...", key="SELECTED_OPENAI_API_KEY"
    )
    if not openai_api_key:
        st.error("Please add your OpenAI API key to continue.")
        st.info("Obtain your key from this link: https://platform.openai.com/account/api-keys")
        st.stop()

    model = "gpt-4.1-mini"
    try:
        client = openai.OpenAI(api_key=openai_api_key)
        available_models = [
            {"id": i.id, "created": datetime.fromtimestamp(i.created)}
            for i in client.models.list()
            if str(i.id).startswith("gpt")
        ]
        available_models = sorted(available_models, key=lambda x: x["created"])
        available_models = [i["id"] for i in available_models]

        model = st.sidebar.selectbox(label="Model", options=available_models, key="SELECTED_OPENAI_MODEL")
    except openai.AuthenticationError as e:
        st.error(e.body["message"])
        st.stop()
    except Exception as e:
        print(e)
        st.error("Something went wrong. Please try again later.")
        st.stop()
    return model, openai_api_key


def configure_llm():
    # available_llms = ["gemini-2.0-flash", "llama-3.1-8b-instant", "llama-3.3-70b-versatile", "gpt-4.1-mini", "groq/compound", "deepseek-r1", "use your openai api key"]
    available_llms = [
        "gemini-2.0-flash",
        "llama-3.1-8b-instant",
        "llama-3.3-70b-versatile",
        "groq/compound",
        "deepseek-r1",
        "use your openai api key",
    ]
    llm_opt = st.sidebar.radio(label="LLM", options=available_llms, key="SELECTED_LLM")

    groq_models = {
        "llama-3.1-8b-instant": "llama-3.1-8b-instant",
        "llama-3.3-70b-versatile": "llama-3.3-70b-versatile",
        "groq/compound": "groq/compound",
        "deepseek-r1": "deepseek-r1-distill-llama-70b",
    }

    if llm_opt in groq_models:
        llm = ChatGroq(model=groq_models[llm_opt], groq_api_key=st.secrets["GROQ_API_KEY"], temperature=0)
    elif llm_opt == "gemini-2.0-flash":
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash", google_api_key=st.secrets["GOOGLE_API_KEY"], temperature=0
        )
    # elif llm_opt == "gpt-4.1-mini":
    #     llm = ChatOpenAI(model_name=llm_opt, temperature=0, streaming=True, api_key=st.secrets["OPENAI_API_KEY"])
    else:
        model, openai_api_key = choose_custom_openai_key()
        llm = ChatOpenAI(model_name=model, temperature=0, streaming=True, api_key=openai_api_key)
    return llm


def print_qa(cls, question, answer):
    log_str = "\nUsecase: {}\nQuestion: {}\nAnswer: {}\n" + "------" * 10
    logger.info(log_str.format(cls.__name__, question, answer))


@st.cache_resource
def configure_embedding_model():
    embedding_model = FastEmbedEmbeddings(model_name="BAAI/bge-small-en-v1.5")
    return embedding_model


def sync_st_session():
    for k, v in st.session_state.items():
        st.session_state[k] = v


def get_current_llm_model():
    """Get the currently selected LLM model name"""
    return st.session_state.get("SELECTED_LLM", "Unknown")


def add_assistant_message_to_history(content):
    """Appends an assistant message to the session state history."""
    current_llm = get_current_llm_model()
    st.session_state.messages.append({"role": "assistant", "content": content, "llm_model": current_llm})
